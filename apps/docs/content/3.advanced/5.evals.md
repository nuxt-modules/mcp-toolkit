---
title: MCP Evals
description: Evaluate MCP tools and workflows with Evalite and the AI SDK MCP client.
navigation:
  icon: i-lucide-flask-conical
seo:
  title: MCP Evals
  description: Evaluate MCP tools and workflows with Evalite and the AI SDK MCP client.
---

## Overview

Evals help you verify that your MCP tools are called correctly by an LLM. This guide shows how to run tool-call evaluations with [Evalite](https://www.evalite.dev/) using the AI SDK MCP client ([docs](https://ai-sdk.dev/docs/reference/ai-sdk-core/create-mcp-client)). The approach stays library-agnosticâ€”Evalite is just the example runner.

## Prerequisites

- An MCP server running locally (e.g., `nuxi dev` with the module enabled)
- Dependencies installed: `evalite`, `@ai-sdk/mcp`, and `ai`
- A model provider key (AI Gateway works well)

Add the env variables used by Evalite and the MCP client:

```bash [.env]
# AI provider (AI Gateway example)
AI_GATEWAY_API_KEY=your_key

# MCP endpoint exposed by your dev server
MCP_URL=http://localhost:3000/mcp
```

## Write an eval

Create an eval file that asks the model to solve a task and checks the tool calls returned by the MCP server.

```typescript [test/mcp.eval.ts]
import { experimental_createMCPClient as createMCPClient } from '@ai-sdk/mcp'
import { generateText } from 'ai'
import { evalite } from 'evalite'
import { toolCallAccuracy } from 'evalite/scorers'

const model = 'openai/gpt-5.1-codex-high' // AI Gateway format
const MCP_URL = process.env.MCP_URL ?? 'http://localhost:3000/mcp'

evalite('BMI Calculator', {
  data: async () => [
    {
      input: 'Calculate BMI for someone who weighs 70kg and is 1.75m tall',
      expected: [{ toolName: 'calculate-bmi', input: { weightKg: 70, heightM: 1.75 } }],
    },
  ],
  task: async (input) => {
    const mcp = await createMCPClient({ transport: { type: 'http', url: MCP_URL } })
    try {
      const result = await generateText({
        model,
        prompt: input,
        tools: await mcp.tools(),
      })
      return result.toolCalls ?? []
    }
    finally {
      await mcp.close()
    }
  },
  scorers: [({ output, expected }) => toolCallAccuracy({ actualCalls: output, expectedCalls: expected })],
})
```

Key points:
- `generateText` receives the MCP tool list and returns tool calls you can score.
- `toolCallAccuracy` checks that the right tool was called with the expected params.
- You can swap `model` for any AI SDK-supported provider.

## Run evals

Run in the terminal (make sure your dev server is up):

```bash
npx evalite
```

Or launch the UI:

```bash
npx evalite --ui
```

## Tips

- Keep prompts specific so the model chooses the intended tool.
- Use realistic inputs that match how users phrase requests.
- For multi-step workflows, increase `maxSteps` in `generateText` and add more `expected` calls.
- Fail fast: start with a few happy-path cases before adding edge cases.
